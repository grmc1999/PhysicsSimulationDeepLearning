{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e951d77-0ee0-4320-8e8b-4fd739711728",
   "metadata": {},
   "source": [
    "# GAN_PI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4c1c928-c7d8-4487-9046-18288c742b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Downloading scipy-1.15.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /dolfinx-env/lib/python3.12/site-packages (from scipy) (2.0.2)\n",
      "Downloading scipy-1.15.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "Successfully installed scipy-1.15.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4865e9b0-c759-451f-a019-96d5da1ef942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e701d86b-911d-45e9-a0ff-5d75b9c3c1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckley_Swc_0_Sor_0_M_2.mat  burgers_shock.mat\n"
     ]
    }
   ],
   "source": [
    "!ls ../../gan-pi/resources/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d3e095b-b0b1-4647-bbb8-1ba25d61a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_lib = os.path.dirname(os.getcwd())\n",
    "path_resources = os.path.join(\"../../gan-pi\", 'resources')\n",
    "path_data = os.path.join(path_resources, 'data')\n",
    "\n",
    "filename = 'Buckley_Swc_0_Sor_0_M_2.mat'\n",
    "path_file = os.path.join(path_data, filename)\n",
    "data = scipy.io.loadmat(path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1788d435-03f2-4f1a-ab17-3e6c9c4e6e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 't', 'usol', 'x'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ef545e8-222a-45b6-8207-391ea7be37a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"usol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grandient example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1.]],\n",
      "\n",
      "        [[1., 1.]],\n",
      "\n",
      "        [[1., 1.]]], requires_grad=True)\n",
      "tensor([[[1.0000, 1.8415, 0.5403, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.8415, 0.5403, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.8415, 0.5403, 1.0000]]], grad_fn=<StackBackward0>)\n",
      "torch.Size([3, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1585, 2.5403]],\n",
       " \n",
       "         [[0.1585, 2.5403]],\n",
       " \n",
       "         [[0.1585, 2.5403]]], grad_fn=<AddBackward0>),)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.ones((3,1,2)).requires_grad_(True)  #\n",
    "print(x1)\n",
    "\n",
    "u1 = torch.stack([\n",
    "    x1[:,:,0]**0.5,\n",
    "    torch.sin(x1[:,:,1])+x1[:,:,0]**0.5,\n",
    "    torch.cos(x1[:,:,0]),\n",
    "    x1[:,:,1]**2,\n",
    "    ],axis=2)\n",
    "\n",
    "#u1 = torch.matmul(M,x1)\n",
    "\n",
    "\n",
    "#u1=torch.sin(torch.matmul(torch.rand(2,2).requires_grad_(True),x1))\n",
    "print(u1)\n",
    "print(u1.shape)\n",
    "torch.autograd.grad(\n",
    "    u1,x1,\n",
    "            grad_outputs=torch.ones_like(u1).to(u1.device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            allow_unused=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian computation\n",
    "\n",
    "Expressed in pytorch with torch.autograd.grad\n",
    "\n",
    "$$ \\textbf{u} \\in \\mathbb{R}^n \\\\ \\textbf{x},\\textbf{e}_i \\in \\mathbb{R}^m $$\n",
    "$$ T_{ag} (\\textbf{u},\\textbf{x},\\textbf{e}_i) = \\nabla_{\\textbf{x}} \\textbf{u}_i  $$\n",
    "$$ T_{ag} (\\textbf{u},\\textbf{x},\\textbf{e}_i) = \\nabla_{\\textbf{x}} \\textbf{u}_i  $$\n",
    "$$ T_{ag} (\\textbf{u},\\textbf{x},\\textbf{e}_i+\\textbf{e}_j) = \\nabla_{\\textbf{x}} \\textbf{u}_i + \\nabla_{\\textbf{x}} \\textbf{u}_j $$\n",
    "$$ [T_{ag} (\\textbf{u},\\textbf{x},\\textbf{e}_i)]_{i=1}^n = J_{\\textbf{x}}(\\textbf{u})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 1.8415, 0.5403, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.8415, 0.5403, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.8415, 0.5403, 1.0000]]], grad_fn=<StackBackward0>)\n",
      "[tensor([[[0.5000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.0000]]], grad_fn=<AddBackward0>), tensor([[[0.5000, 0.5403]],\n",
      "\n",
      "        [[0.5000, 0.5403]],\n",
      "\n",
      "        [[0.5000, 0.5403]]], grad_fn=<AddBackward0>), tensor([[[-0.8415,  0.0000]],\n",
      "\n",
      "        [[-0.8415,  0.0000]],\n",
      "\n",
      "        [[-0.8415,  0.0000]]], grad_fn=<AddBackward0>), tensor([[[0., 2.]],\n",
      "\n",
      "        [[0., 2.]],\n",
      "\n",
      "        [[0., 2.]]], grad_fn=<AddBackward0>)]\n",
      "tensor([[[[ 0.5000,  0.0000],\n",
      "          [ 0.5000,  0.5403],\n",
      "          [-0.8415,  0.0000],\n",
      "          [ 0.0000,  2.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5000,  0.0000],\n",
      "          [ 0.5000,  0.5403],\n",
      "          [-0.8415,  0.0000],\n",
      "          [ 0.0000,  2.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5000,  0.0000],\n",
      "          [ 0.5000,  0.5403],\n",
      "          [-0.8415,  0.0000],\n",
      "          [ 0.0000,  2.0000]]]], grad_fn=<StackBackward0>)\n",
      "torch.Size([3, 1, 4, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 2])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unit_vectors=torch.eye(4)\n",
    "print(u1)\n",
    "jacobian_rows = [torch.autograd.grad(u1, x1, vec_.unsqueeze(0).unsqueeze(0).tile(u1.shape[0],u1.shape[1],1),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            allow_unused=True)[0]\n",
    "                     for vec_ in unit_vectors]\n",
    "print(jacobian_rows)\n",
    "print(torch.stack(jacobian_rows,axis=2))\n",
    "print(torch.stack(jacobian_rows,axis=2).shape)\n",
    "\n",
    "torch.autograd.grad(u1, x1, unit_vectors[1].unsqueeze(0).unsqueeze(0).tile(u1.shape[0],u1.shape[1],1),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            allow_unused=True)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def vector_jacobian(u,x):\n",
    "    unit_vectors=torch.eye(u.shape[-1])\n",
    "    jacobian_rows = [torch.autograd.grad(u, x, vec_.unsqueeze(0).unsqueeze(0).tile(u.shape[0],u.shape[1],1),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            allow_unused=True)[0]\n",
    "                     for vec_ in unit_vectors]\n",
    "    return torch.stack(jacobian_rows,axis=2)\n",
    "\n",
    "\n",
    "def vector_grad(u,x):\n",
    "    unit_vectors=torch.eye(u.shape[-1])\n",
    "    jacobian_rows = [torch.autograd.grad(u, x, vec_.unsqueeze(0).unsqueeze(0).tile(u.shape[0],u.shape[1],1),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            allow_unused=True)[0]\n",
    "                     for vec_ in unit_vectors]\n",
    "    return torch.diagonal(torch.stack(jacobian_rows,axis=2),dim1=-2,dim2=-1)\n",
    "\n",
    "def x_grad(u,x,i,n):\n",
    "    \"\"\"\n",
    "    gradient of degree wrt x for componen i for u\n",
    "    input:\n",
    "    u and x are tensors with vectors object at dimension -1\n",
    "    [b, n_vectors, vector_dimension]\n",
    "\n",
    "    output:\n",
    "    [b, n_vectors, input_vector_dimension]\n",
    "    \"\"\"\n",
    "    I=torch.eye(u.shape[-1])\n",
    "\n",
    "    u=torch.autograd.grad(u ,x,\n",
    "            I[i].unsqueeze(0).unsqueeze(0).tile(u.shape[0],u.shape[1],1).to(u.device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            allow_unused=True)[0]\n",
    "    if n > 1:\n",
    "        for i in range(n-1):\n",
    "            u=vector_grad(u,x)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3.2812, 0.0000]],\n",
      "\n",
      "        [[3.2812, 0.0000]],\n",
      "\n",
      "        [[3.2812, 0.0000]]], grad_fn=<DiagonalBackward0>)\n",
      "torch.Size([3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x_grad(u1,x1,0,5))\n",
    "print(x_grad(u1,x1,3,4).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "diagonal(input, offset=0, dim1=0, dim2=1) -> Tensor\n",
      "\n",
      "Returns a partial view of :attr:`input` with the its diagonal elements\n",
      "with respect to :attr:`dim1` and :attr:`dim2` appended as a dimension\n",
      "at the end of the shape.\n",
      "\n",
      "The argument :attr:`offset` controls which diagonal to consider:\n",
      "\n",
      "- If :attr:`offset` = 0, it is the main diagonal.\n",
      "- If :attr:`offset` > 0, it is above the main diagonal.\n",
      "- If :attr:`offset` < 0, it is below the main diagonal.\n",
      "\n",
      "Applying :meth:`torch.diag_embed` to the output of this function with\n",
      "the same arguments yields a diagonal matrix with the diagonal entries\n",
      "of the input. However, :meth:`torch.diag_embed` has different default\n",
      "dimensions, so those need to be explicitly specified.\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor. Must be at least 2-dimensional.\n",
      "    offset (int, optional): which diagonal to consider. Default: 0\n",
      "        (main diagonal).\n",
      "    dim1 (int, optional): first dimension with respect to which to\n",
      "        take diagonal. Default: 0.\n",
      "    dim2 (int, optional): second dimension with respect to which to\n",
      "        take diagonal. Default: 1.\n",
      "\n",
      ".. note::  To take a batch diagonal, pass in dim1=-2, dim2=-1.\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> a = torch.randn(3, 3)\n",
      "    >>> a\n",
      "    tensor([[-1.0854,  1.1431, -0.1752],\n",
      "            [ 0.8536, -0.0905,  0.0360],\n",
      "            [ 0.6927, -0.3735, -0.4945]])\n",
      "\n",
      "\n",
      "    >>> torch.diagonal(a)\n",
      "    tensor([-1.0854, -0.0905, -0.4945])\n",
      "\n",
      "\n",
      "    >>> torch.diagonal(a, 1)\n",
      "    tensor([ 1.1431,  0.0360])\n",
      "\n",
      "    >>> b = torch.randn(2, 5)\n",
      "    >>> b\n",
      "    tensor([[-1.7948, -1.2731, -0.3181,  2.0200, -1.6745],\n",
      "            [ 1.8262, -1.5049,  0.4114,  1.0704, -1.2607]])\n",
      "\n",
      "    >>> torch.diagonal(b, 1, 1, 0)\n",
      "    tensor([1.8262])\n",
      "\n",
      "    >>> x = torch.randn(2, 5, 4, 2)\n",
      "    >>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)\n",
      "    tensor([[[-1.2631,  0.3755, -1.5977, -1.8172],\n",
      "             [-1.1065,  1.0401, -0.2235, -0.7938]],\n",
      "\n",
      "            [[-1.7325, -0.3081,  0.6166,  0.2335],\n",
      "             [ 1.0500,  0.7336, -0.3836, -1.1015]]])\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "torch.diagonal?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
